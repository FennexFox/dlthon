{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80b69344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/technocracy90/keras/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1737629942.635320    8076 service.cc:148] XLA service 0x7f867c0060f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1737629942.635773    8076 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 3080, Compute Capability 8.6\n",
      "2025-01-23 19:59:02.653719: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1737629942.697961    8076 cuda_dnn.cc:529] Loaded cuDNN version 90600\n",
      "2025-01-23 19:59:03.610303: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_542_0', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-01-23 19:59:03.843944: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_526_0', 200 bytes spill stores, 200 bytes spill loads\n",
      "\n",
      "2025-01-23 19:59:04.002920: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_526', 44 bytes spill stores, 44 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m41/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1284 - loss: 2.3860"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1737629944.996011    8076 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2025-01-23 19:59:06.003132: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_526_0', 192 bytes spill stores, 192 bytes spill loads\n",
      "\n",
      "2025-01-23 19:59:06.202683: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_526', 44 bytes spill stores, 44 bytes spill loads\n",
      "\n",
      "2025-01-23 19:59:06.294236: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_542', 180 bytes spill stores, 180 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - accuracy: 0.1356 - loss: 2.3664 - val_accuracy: 0.5740 - val_loss: 1.8697\n",
      "Epoch 2/8\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3591 - loss: 1.9035 - val_accuracy: 0.7200 - val_loss: 1.5478\n",
      "Epoch 3/8\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.4985 - loss: 1.6239 - val_accuracy: 0.7760 - val_loss: 1.3073\n",
      "Epoch 4/8\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5948 - loss: 1.4106 - val_accuracy: 0.7940 - val_loss: 1.1290\n",
      "Epoch 5/8\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6449 - loss: 1.2563 - val_accuracy: 0.8020 - val_loss: 0.9974\n",
      "Epoch 6/8\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6711 - loss: 1.1357 - val_accuracy: 0.8100 - val_loss: 0.8960\n",
      "Epoch 7/8\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7073 - loss: 1.0439 - val_accuracy: 0.8120 - val_loss: 0.8210\n",
      "Epoch 8/8\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7270 - loss: 0.9640 - val_accuracy: 0.8300 - val_loss: 0.7605\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch/accuracy</td><td>▁▁▂▂▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇████████</td></tr><tr><td>batch/batch_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>batch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch/loss</td><td>██▇▇▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/accuracy</td><td>▁▄▅▆▇▇██</td></tr><tr><td>epoch/epoch</td><td>▁▂▃▄▅▆▇█</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▆▄▃▂▂▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▅▇▇▇▇██</td></tr><tr><td>epoch/val_loss</td><td>█▆▄▃▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch/accuracy</td><td>0.7269</td></tr><tr><td>batch/batch_step</td><td>395</td></tr><tr><td>batch/learning_rate</td><td>0.01</td></tr><tr><td>batch/loss</td><td>0.95456</td></tr><tr><td>epoch/accuracy</td><td>0.72592</td></tr><tr><td>epoch/epoch</td><td>7</td></tr><tr><td>epoch/learning_rate</td><td>0.01</td></tr><tr><td>epoch/loss</td><td>0.95542</td></tr><tr><td>epoch/val_accuracy</td><td>0.83</td></tr><tr><td>epoch/val_loss</td><td>0.76051</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">tough-butterfly-1</strong> at: <a href='https://wandb.ai/technocracy90-aiffel/my-awesome-project/runs/s3nvkoc3' target=\"_blank\">https://wandb.ai/technocracy90-aiffel/my-awesome-project/runs/s3nvkoc3</a><br> View project at: <a href='https://wandb.ai/technocracy90-aiffel/my-awesome-project' target=\"_blank\">https://wandb.ai/technocracy90-aiffel/my-awesome-project</a><br>Synced 5 W&B file(s), 0 media file(s), 16 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250123_195837-s3nvkoc3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This script needs these libraries to be installed:\n",
    "#   tensorflow, numpy\n",
    "\n",
    "import wandb\n",
    "from wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Start a run, tracking hyperparameters\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"my-awesome-project\",\n",
    "\n",
    "    # track hyperparameters and run metadata with wandb.config\n",
    "    config={\n",
    "        \"layer_1\": 512,\n",
    "        \"activation_1\": \"relu\",\n",
    "        \"dropout\": random.uniform(0.01, 0.80),\n",
    "        \"layer_2\": 10,\n",
    "        \"activation_2\": \"softmax\",\n",
    "        \"optimizer\": \"sgd\",\n",
    "        \"loss\": \"sparse_categorical_crossentropy\",\n",
    "        \"metric\": \"accuracy\",\n",
    "        \"epoch\": 8,\n",
    "        \"batch_size\": 256\n",
    "    }\n",
    ")\n",
    "\n",
    "# [optional] use wandb.config as your config\n",
    "config = wandb.config\n",
    "\n",
    "# get the data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "x_train, y_train = x_train[::5], y_train[::5]\n",
    "x_test, y_test = x_test[::20], y_test[::20]\n",
    "labels = [str(digit) for digit in range(np.max(y_train) + 1)]\n",
    "\n",
    "# build a model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(config.layer_1, activation=config.activation_1),\n",
    "    tf.keras.layers.Dropout(config.dropout),\n",
    "    tf.keras.layers.Dense(config.layer_2, activation=config.activation_2)\n",
    "    ])\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=config.optimizer,\n",
    "              loss=config.loss,\n",
    "              metrics=[config.metric]\n",
    "              )\n",
    "\n",
    "# WandbMetricsLogger will log train and validation metrics to wandb\n",
    "# WandbModelCheckpoint will upload model checkpoints to wandb\n",
    "history = model.fit(x=x_train, y=y_train,\n",
    "                    epochs=config.epoch,\n",
    "                    batch_size=config.batch_size,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=[\n",
    "                      WandbMetricsLogger(log_freq=5),\n",
    "                      WandbModelCheckpoint(\"models.keras\", )\n",
    "                    ])\n",
    "\n",
    "# [optional] finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
